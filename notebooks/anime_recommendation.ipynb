{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Install Required Packages"
      ],
      "metadata": {
        "id": "vbYYLiUN4Toa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGbBs59M3yE4"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages using pip\n",
        "!pip install scikit-surprise\n",
        "!pip install --upgrade scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Libraries"
      ],
      "metadata": {
        "id": "JER-Dhm94YNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries for data processing, visualization, and modeling\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# For text processing in content-based filtering\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# For collaborative filtering using the Surprise library\n",
        "from surprise import Dataset, Reader, SVD, KNNBasic\n",
        "from surprise.model_selection import cross_validate, train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSEhHuRl4Z0D",
        "outputId": "a495d508-9e0a-4552-e284-452f9568e4dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clone Github Repo"
      ],
      "metadata": {
        "id": "4J40bdrk5c2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the GitHub repository\n",
        "!git clone https://github.com/SomersInias/AI-Anime-Recommendation.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_YIEEdg5fpM",
        "outputId": "f4bdbc70-d22c-4747-80a1-5f03af22a120"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AI-Anime-Recommendation'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 9 (delta 0), reused 6 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (9/9), 25.10 MiB | 9.59 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Ingestion – Load the Datasets"
      ],
      "metadata": {
        "id": "m4dFyjJ64buE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets.\n",
        "# Make sure you have uploaded 'anime.csv' and 'rating.csv' to your Colab environment or google colab\n",
        "\n",
        "anime_df = pd.read_csv('/content/AI-Anime-Recommendation/datasets/anime.csv')\n",
        "rating_df = pd.read_csv('/content/AI-Anime-Recommendation/datasets/rating.csv')\n",
        "\n",
        "print(\"Anime dataset shape:\", anime_df.shape)\n",
        "print(\"Rating dataset shape:\", rating_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDYNGmFn4dhk",
        "outputId": "3a333d93-5015-4fcf-ba13-c26f96a742aa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anime dataset shape: (12294, 7)\n",
            "Rating dataset shape: (7813737, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Cleaning and Preprocessing"
      ],
      "metadata": {
        "id": "AX26iEa-4mw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Cleaning:\n",
        "# - Remove duplicate rows.\n",
        "# - For ratings, remove entries where rating == -1 (if used as a flag for 'not rated').\n",
        "# - Fill missing values in key columns.\n",
        "\n",
        "anime_df.drop_duplicates(inplace=True)\n",
        "rating_df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Remove rows where the rating is -1 (if applicable)\n",
        "rating_df = rating_df[rating_df['rating'] != -1]\n",
        "\n",
        "# Fill missing genres with an empty string.\n",
        "anime_df['genre'] = anime_df['genre'].fillna('')\n",
        "\n",
        "print(\"Cleaned anime dataset shape:\", anime_df.shape)\n",
        "print(\"Cleaned rating dataset shape:\", rating_df.shape)"
      ],
      "metadata": {
        "id": "JHtiD5RD4oTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "8ASepXMs4ptw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of user ratings from the rating dataset.\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(rating_df['rating'], bins=20, kde=True, color='skyblue')\n",
        "plt.title(\"Distribution of User Ratings\")\n",
        "plt.xlabel(\"Rating\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "# Plot the distribution of anime ratings from the anime metadata.\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(anime_df['rating'], bins=20, kde=True, color='salmon')\n",
        "plt.title(\"Distribution of Anime Ratings (Metadata)\")\n",
        "plt.xlabel(\"Rating\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "# Analyze the top genres:\n",
        "# Split the 'genre' column by commas, explode the list, and count the occurrences.\n",
        "\n",
        "anime_genres = anime_df['genre'].str.split(',').explode().str.strip()\n",
        "genre_counts = anime_genres[anime_genres != ''].value_counts().head(10)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=genre_counts.values, y=genre_counts.index, palette='viridis')\n",
        "plt.title(\"Top 10 Anime Genres\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Genre\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J35KKcfJ4rph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build the Collaborative Filtering Model (SVD)"
      ],
      "metadata": {
        "id": "Ithn6fUG4uW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Reader object for the Surprise library.\n",
        "# The rating scale here is assumed to be from 1 to 10.\n",
        "\n",
        "reader = Reader(rating_scale=(1, 10))\n",
        "\n",
        "# Load the rating data into Surprise's Dataset format.\n",
        "data = Dataset.load_from_df(rating_df[['user_id', 'anime_id', 'rating']], reader)\n",
        "\n",
        "# Evaluate the SVD model using 5-fold cross-validation.\n",
        "svd = SVD()\n",
        "cv_results = cross_validate(svd, data, measures=['RMSE'], cv=5, verbose=True)\n",
        "\n",
        "# Train the SVD model on the full dataset.\n",
        "trainset = data.build_full_trainset()\n",
        "svd.fit(trainset)\n",
        "print(\"SVD model training complete.\")"
      ],
      "metadata": {
        "id": "UeqF5cfb4w2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Collaborative Filtering – Generate Recommendations Using SVD"
      ],
      "metadata": {
        "id": "ca8QpQzk4zE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate SVD-based recommendations for a given user_id.\n",
        "def get_svd_recommendations(user_id, n=10):\n",
        "    # Get a list of all unique anime_ids in the ratings dataset.\n",
        "    all_anime_ids = rating_df['anime_id'].unique()\n",
        "    # Find the anime_ids that the user has already rated.\n",
        "    rated_anime = rating_df[rating_df['user_id'] == user_id]['anime_id'].tolist()\n",
        "    # The candidates for recommendation are those anime that the user hasn't rated.\n",
        "    candidates = [aid for aid in all_anime_ids if aid not in rated_anime]\n",
        "\n",
        "    predictions = []\n",
        "    for aid in candidates:\n",
        "        pred = svd.predict(user_id, aid)\n",
        "        predictions.append((aid, pred.est))\n",
        "\n",
        "    # Sort candidates based on the predicted rating (highest first).\n",
        "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "    top_n = predictions[:n]\n",
        "\n",
        "    # Build a DataFrame with the recommended anime names and predicted ratings.\n",
        "    recs = []\n",
        "    for aid, pred_rating in top_n:\n",
        "        anime_name = anime_df[anime_df['anime_id'] == aid]['name'].values\n",
        "        if len(anime_name) > 0:\n",
        "            recs.append({'anime_id': aid, 'name': anime_name[0], 'predicted_rating': pred_rating})\n",
        "    return pd.DataFrame(recs)\n",
        "\n",
        "# Test the SVD recommendations for a sample user.\n",
        "sample_user = rating_df['user_id'].sample(1).iloc[0]\n",
        "print(\"Generating SVD recommendations for user_id:\", sample_user)\n",
        "svd_recs = get_svd_recommendations(sample_user, n=10)\n",
        "svd_recs"
      ],
      "metadata": {
        "id": "j3aTluKN43rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate SVD-based recommendations for a given user_id.\n",
        "def get_svd_recommendations(user_id, n=10):\n",
        "    # Get a list of all unique anime_ids in the ratings dataset.\n",
        "    all_anime_ids = rating_df['anime_id'].unique()\n",
        "    # Find the anime_ids that the user has already rated.\n",
        "    rated_anime = rating_df[rating_df['user_id'] == user_id]['anime_id'].tolist()\n",
        "    # The candidates for recommendation are those anime that the user hasn't rated.\n",
        "    candidates = [aid for aid in all_anime_ids if aid not in rated_anime]\n",
        "\n",
        "    predictions = []\n",
        "    for aid in candidates:\n",
        "        pred = svd.predict(user_id, aid)\n",
        "        predictions.append((aid, pred.est))\n",
        "\n",
        "    # Sort candidates based on the predicted rating (highest first).\n",
        "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "    top_n = predictions[:n]\n",
        "\n",
        "    # Build a DataFrame with the recommended anime names and predicted ratings.\n",
        "    recs = []\n",
        "    for aid, pred_rating in top_n:\n",
        "        anime_name = anime_df[anime_df['anime_id'] == aid]['name'].values\n",
        "        if len(anime_name) > 0:\n",
        "            recs.append({'anime_id': aid, 'name': anime_name[0], 'predicted_rating': pred_rating})\n",
        "    return pd.DataFrame(recs)\n",
        "\n",
        "# Test the SVD recommendations for a sample user.\n",
        "sample_user = rating_df['user_id'].sample(1).iloc[0]\n",
        "print(\"Generating SVD recommendations for user_id:\", sample_user)\n",
        "svd_recs = get_svd_recommendations(sample_user, n=10)\n",
        "svd_recs"
      ],
      "metadata": {
        "id": "5KPu7E6J45Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build the Content-Based Filtering Model Using TF-IDF"
      ],
      "metadata": {
        "id": "LW3Di6lw465O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For content-based filtering, we vectorize the 'genre' column using TF-IDF.\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf.fit_transform(anime_df['genre'])\n",
        "\n",
        "# Compute the cosine similarity matrix for the anime based on their TF-IDF vectors.\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "print(\"Cosine similarity matrix computed.\")\n",
        "\n",
        "# Create a reverse mapping from anime name to its index for easy lookup.\n",
        "indices = pd.Series(anime_df.index, index=anime_df['name']).drop_duplicates()\n"
      ],
      "metadata": {
        "id": "yyNEJV6L4-yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Content-Based Filtering – Generate Recommendations Based on an Anime Title"
      ],
      "metadata": {
        "id": "N1E1aB0_5ASR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get content-based recommendations for a given anime title.\n",
        "def get_content_recommendations(title, n=10):\n",
        "    if title not in indices:\n",
        "        print(\"Anime title not found in the dataset.\")\n",
        "        return None\n",
        "    idx = indices[title]\n",
        "    # Compute pairwise similarity scores for the selected anime.\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    # Sort the anime based on similarity scores (highest first) and skip the first one (the anime itself).\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:n+1]\n",
        "    anime_indices = [i[0] for i in sim_scores]\n",
        "    return anime_df.iloc[anime_indices][['anime_id', 'name', 'genre', 'rating']]\n",
        "\n",
        "# Test content-based recommendations for a sample anime title.\n",
        "sample_title = anime_df['name'].iloc[0]\n",
        "print(\"Generating content-based recommendations for:\", sample_title)\n",
        "content_recs = get_content_recommendations(sample_title, n=10)\n",
        "content_recs\n"
      ],
      "metadata": {
        "id": "zHNMeM5b5Byp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hybrid Recommendation Approach"
      ],
      "metadata": {
        "id": "so4mOxJK5D45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A simple hybrid approach combines the SVD predicted ratings (collaborative) with content similarity.\n",
        "# 'alpha' determines the weight for the collaborative part (with 1 - alpha for content similarity).\n",
        "\n",
        "def hybrid_recommendations(user_id, anime_title, alpha=0.5, n=10):\n",
        "    \"\"\"\n",
        "    Returns hybrid recommendations based on:\n",
        "    - SVD collaborative filtering predictions.\n",
        "    - Cosine similarity with a given anime title.\n",
        "\n",
        "    Parameters:\n",
        "        user_id: The ID of the user for whom to generate recommendations.\n",
        "        anime_title: The anime title used as context for content similarity.\n",
        "        alpha: Weight for collaborative filtering score (0 <= alpha <= 1).\n",
        "        n: Number of recommendations to return.\n",
        "    \"\"\"\n",
        "    # Get collaborative filtering recommendations (many candidates).\n",
        "    svd_recs = get_svd_recommendations(user_id, n=500)\n",
        "\n",
        "    # Get index of the context anime for content-based similarity.\n",
        "    if anime_title not in indices:\n",
        "        print(\"Anime title not found for content-based filtering.\")\n",
        "        return None\n",
        "    title_idx = indices[anime_title]\n",
        "\n",
        "    scores = []\n",
        "    # For each candidate from SVD, compute a hybrid score.\n",
        "    for _, row in svd_recs.iterrows():\n",
        "        candidate_idx = anime_df[anime_df['anime_id'] == row['anime_id']].index[0]\n",
        "        content_score = cosine_sim[title_idx][candidate_idx]\n",
        "        # Normalize the collaborative score (assuming ratings range from 1 to 10).\n",
        "        collab_score = (row['predicted_rating'] - 1) / 9\n",
        "        hybrid_score = alpha * collab_score + (1 - alpha) * content_score\n",
        "        scores.append((row['anime_id'], row['name'], hybrid_score))\n",
        "\n",
        "    # Sort the candidates by the hybrid score (highest first).\n",
        "    scores.sort(key=lambda x: x[2], reverse=True)\n",
        "    top_n = scores[:n]\n",
        "    return pd.DataFrame(top_n, columns=['anime_id', 'name', 'hybrid_score'])\n",
        "\n",
        "# Test the hybrid recommendation function.\n",
        "print(\"Hybrid Recommendations for user_id {} with context '{}':\".format(sample_user, sample_title))\n",
        "hybrid_recs = hybrid_recommendations(sample_user, sample_title, alpha=0.5, n=10)\n",
        "hybrid_recs\n"
      ],
      "metadata": {
        "id": "WFsXv1qn5FAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create new user"
      ],
      "metadata": {
        "id": "0wwsIRL8E6e_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_test_user(anime_ratings, user_id=None):\n",
        "    \"\"\"\n",
        "    Create a test user with custom ratings for multiple anime and add the user to the global rating_df.\n",
        "\n",
        "    Parameters:\n",
        "      - anime_ratings (dict): Dictionary where keys are anime titles and values are ratings.\n",
        "      - user_id (int, optional): If provided, uses this user_id; otherwise, generates a new unique user_id.\n",
        "\n",
        "    Returns:\n",
        "      - DataFrame: A DataFrame containing only the new test user's ratings.\n",
        "    \"\"\"\n",
        "    global rating_df  # We'll update the global rating_df\n",
        "\n",
        "    # Generate a new unique user_id if not provided\n",
        "    if user_id is None:\n",
        "        if rating_df.empty:\n",
        "            user_id = 1\n",
        "        else:\n",
        "            user_id = rating_df['user_id'].max() + 1\n",
        "\n",
        "    new_ratings = []\n",
        "    for title, rating in anime_ratings.items():\n",
        "        # Check if the anime exists in anime_df.\n",
        "        anime_entry = anime_df[anime_df['name'] == title]\n",
        "        if anime_entry.empty:\n",
        "            print(f\"Anime '{title}' not found in the dataset. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        anime_id = anime_entry['anime_id'].values[0]\n",
        "        new_ratings.append({\n",
        "            'user_id': user_id,  # All ratings will share this test user ID.\n",
        "            'anime_id': anime_id,\n",
        "            'rating': rating\n",
        "        })\n",
        "\n",
        "    if not new_ratings:\n",
        "        print(\"No valid anime found. No test user created.\")\n",
        "        return None\n",
        "\n",
        "    new_ratings_df = pd.DataFrame(new_ratings)\n",
        "\n",
        "    # Append the new test user's ratings to the global rating_df.\n",
        "    rating_df = pd.concat([rating_df, new_ratings_df], ignore_index=True)\n",
        "\n",
        "    print(f\"Created test user with user_id {user_id}\")\n",
        "    return new_ratings_df"
      ],
      "metadata": {
        "id": "Uil0NoOEE8Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "test_anime_ratings = {\n",
        "    \"Naruto\": 0,\n",
        "    \"One Piece\": 0,\n",
        "    \"Attack on Titan\": 8.5  # Ensure the title exactly matches an entry in anime_df.\n",
        "}\n",
        "\n",
        "new_test_user_ratings = create_test_user(test_anime_ratings)\n",
        "print(new_test_user_ratings)"
      ],
      "metadata": {
        "id": "02NCyqfIFAdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Display Final Recommendations\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bVNAoznz5IZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the recommendations from each method.\n",
        "\n",
        "print(\"SVD (Collaborative Filtering) Recommendations for user_id {}:\".format(sample_user))\n",
        "display(svd_recs)\n",
        "\n",
        "print(\"\\nContent-Based Recommendations for anime '{}':\".format(sample_title))\n",
        "display(content_recs)\n",
        "\n",
        "print(\"\\nHybrid Recommendations for user_id {} with context '{}':\".format(sample_user, sample_title))\n",
        "display(hybrid_recs)\n"
      ],
      "metadata": {
        "id": "bTPE27Eo5Ja0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train models and upload to google drive"
      ],
      "metadata": {
        "id": "HzB5REug6aF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from surprise import Dataset, Reader, SVD\n",
        "import joblib\n",
        "\n",
        "# Train SVD model (same as in your notebook)\n",
        "reader = Reader(rating_scale=(1, 10))\n",
        "data = Dataset.load_from_df(rating_df[['user_id', 'anime_id', 'rating']], reader)\n",
        "trainset = data.build_full_trainset()\n",
        "svd_model = SVD()\n",
        "svd_model.fit(trainset)\n",
        "\n",
        "# Compute TF-IDF matrix, cosine similarity, and indices (same as in your notebook)\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf.fit_transform(anime_df['genre'])\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "indices = pd.Series(anime_df.index, index=anime_df['name']).drop_duplicates()\n",
        "\n",
        "# Save the trained models and components\n",
        "joblib.dump(svd_model, '/content/drive/MyDrive/svd_model.joblib')\n",
        "joblib.dump(tfidf, '/content/drive/MyDrive/tfidf_vectorizer.joblib')\n",
        "joblib.dump(cosine_sim, '/content/drive/MyDrive/cosine_sim_matrix.joblib')\n",
        "joblib.dump(indices, '/content/drive/MyDrive/indices.joblib')\n",
        "\n",
        "print(\"Models and components saved successfully!\")"
      ],
      "metadata": {
        "id": "wNCVWkq96lqj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}